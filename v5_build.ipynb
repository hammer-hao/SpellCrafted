{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model params\n",
    "torch.manual_seed(69)\n",
    "batch_size=512\n",
    "block_size=36\n",
    "sampling_size=24\n",
    "max_iters=5000\n",
    "eval_interval=300\n",
    "learning_rate=5e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 32\n",
    "n_heads = 8\n",
    "n_layers = 10\n",
    "dropout=0.3\n",
    "\n",
    "prompt_size = block_size # 30 maximum tokens allowed in the prompt\n",
    "encoder_num_heads=4\n",
    "encoder_n_embd=n_embd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer\n",
    "\n",
    "Run only one of the two cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tokenizer trainer\n",
    "from tokenizers import Tokenizer\n",
    "tokenizer = Tokenizer.from_file(\"mtggenerator.json\")\n",
    "vocab_size=tokenizer.get_vocab_size()\n",
    "\n",
    "#create the mapping from characters to integers\n",
    "encode = lambda text: tokenizer.encode(text).ids #encode: take a string, output a list of integers\n",
    "decode = lambda list: tokenizer.decode(list) #decode: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hammerhao\\OneDrive\\Documents\\GitHub\\SpellCrafted\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "vocab_size=tokenizer.vocab_size\n",
    "\n",
    "#create the mapping from characters to integers\n",
    "encode = lambda text: tokenizer.encode(text) #encode: take a string, output a list of integers\n",
    "decode = lambda list: tokenizer.decode(list) #decode: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>mana_cost</th>\n",
       "      <th>cmc</th>\n",
       "      <th>type_line</th>\n",
       "      <th>oracle_text</th>\n",
       "      <th>power</th>\n",
       "      <th>toughness</th>\n",
       "      <th>colors</th>\n",
       "      <th>color_identity</th>\n",
       "      <th>keywords</th>\n",
       "      <th>rarity</th>\n",
       "      <th>flavor_text</th>\n",
       "      <th>text</th>\n",
       "      <th>text_prompt</th>\n",
       "      <th>card_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fury Sliver</td>\n",
       "      <td>{5}{R}</td>\n",
       "      <td>6.0</td>\n",
       "      <td>Creature — Sliver</td>\n",
       "      <td>All Sliver creatures have double strike.</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>['R']</td>\n",
       "      <td>['R']</td>\n",
       "      <td>[]</td>\n",
       "      <td>uncommon</td>\n",
       "      <td>\"A rift opened, and our arrows were abruptly s...</td>\n",
       "      <td>Fury Sliver: [SEP] {5}{R} [SEP] Creature — Sli...</td>\n",
       "      <td>Fury Sliver: [SEP] {5}{R}</td>\n",
       "      <td>Creature — Sliver [SEP] All Sliver creatures h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kor Outfitter</td>\n",
       "      <td>{W}{W}</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Creature — Kor Soldier</td>\n",
       "      <td>When ~ enters the battlefield, you may attach ...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>['W']</td>\n",
       "      <td>['W']</td>\n",
       "      <td>[]</td>\n",
       "      <td>common</td>\n",
       "      <td>\"We take only what we need to survive. Believe...</td>\n",
       "      <td>Kor Outfitter: [SEP] {W}{W} [SEP] Creature — K...</td>\n",
       "      <td>Kor Outfitter: [SEP] {W}{W}</td>\n",
       "      <td>Creature — Kor Soldier [SEP] When ~ enters the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spirit</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Token Creature — Spirit</td>\n",
       "      <td>Flying</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['W']</td>\n",
       "      <td>['W']</td>\n",
       "      <td>[Flying]</td>\n",
       "      <td>common</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Spirit: [SEP]  [SEP] Token Creature — Spirit [...</td>\n",
       "      <td>Spirit: [SEP]</td>\n",
       "      <td>Token Creature — Spirit [SEP] Flying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Siren Lookout</td>\n",
       "      <td>{2}{U}</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Creature — Siren Pirate</td>\n",
       "      <td>Flying\\nWhen ~ enters the battlefield, it expl...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>['U']</td>\n",
       "      <td>['U']</td>\n",
       "      <td>[Flying, Explore]</td>\n",
       "      <td>common</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Siren Lookout: [SEP] {2}{U} [SEP] Creature — S...</td>\n",
       "      <td>Siren Lookout: [SEP] {2}{U}</td>\n",
       "      <td>Creature — Siren Pirate [SEP] Flying\\nWhen ~ e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Web</td>\n",
       "      <td>{G}</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Enchantment — Aura</td>\n",
       "      <td>Enchant creature (Target a creature as you cas...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['G']</td>\n",
       "      <td>['G']</td>\n",
       "      <td>[Enchant]</td>\n",
       "      <td>rare</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Web: [SEP] {G} [SEP] Enchantment — Aura [SEP] ...</td>\n",
       "      <td>Web: [SEP] {G}</td>\n",
       "      <td>Enchantment — Aura [SEP] Enchant creature (Tar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85059</th>\n",
       "      <td>Celestine Reef</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Plane — Luvion</td>\n",
       "      <td>Creatures without flying or islandwalk can't a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>rare</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Celestine Reef: [SEP]  [SEP] Plane — Luvion [S...</td>\n",
       "      <td>Celestine Reef: [SEP]</td>\n",
       "      <td>Plane — Luvion [SEP] Creatures without flying ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85060</th>\n",
       "      <td>Horned Troll</td>\n",
       "      <td>{2}{G}</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Creature — Troll</td>\n",
       "      <td>{G}: Regenerate ~.</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>['G']</td>\n",
       "      <td>['G']</td>\n",
       "      <td>[]</td>\n",
       "      <td>common</td>\n",
       "      <td>Sword hilts jut from some trolls' bodies where...</td>\n",
       "      <td>Horned Troll: [SEP] {2}{G} [SEP] Creature — Tr...</td>\n",
       "      <td>Horned Troll: [SEP] {2}{G}</td>\n",
       "      <td>Creature — Troll [SEP] {G}: Regenerate ~.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85061</th>\n",
       "      <td>Faerie Bladecrafter</td>\n",
       "      <td>{2}{B}</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Creature — Faerie Rogue</td>\n",
       "      <td>Flying\\nWhenever one or more Faeries you contr...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>['B']</td>\n",
       "      <td>['B']</td>\n",
       "      <td>[Flying]</td>\n",
       "      <td>rare</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Faerie Bladecrafter: [SEP] {2}{B} [SEP] Creatu...</td>\n",
       "      <td>Faerie Bladecrafter: [SEP] {2}{B}</td>\n",
       "      <td>Creature — Faerie Rogue [SEP] Flying\\nWhenever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85062</th>\n",
       "      <td>Exultant Skymarcher</td>\n",
       "      <td>{1}{W}{W}</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Creature — Vampire Soldier</td>\n",
       "      <td>Flying</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>['W']</td>\n",
       "      <td>['W']</td>\n",
       "      <td>[Flying]</td>\n",
       "      <td>common</td>\n",
       "      <td>\"We have come at last to this holiest of holy ...</td>\n",
       "      <td>Exultant Skymarcher: [SEP] {1}{W}{W} [SEP] Cre...</td>\n",
       "      <td>Exultant Skymarcher: [SEP] {1}{W}{W}</td>\n",
       "      <td>Creature — Vampire Soldier [SEP] Flying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85063</th>\n",
       "      <td>Disintegrate</td>\n",
       "      <td>{X}{R}</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Sorcery</td>\n",
       "      <td>~ deals X damage to any target. If it's a crea...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['R']</td>\n",
       "      <td>['R']</td>\n",
       "      <td>[]</td>\n",
       "      <td>common</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Disintegrate: [SEP] {X}{R} [SEP] Sorcery [SEP]...</td>\n",
       "      <td>Disintegrate: [SEP] {X}{R}</td>\n",
       "      <td>Sorcery [SEP] ~ deals X damage to any target. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>82351 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      name  mana_cost  cmc                   type_line  \\\n",
       "0              Fury Sliver     {5}{R}  6.0           Creature — Sliver   \n",
       "1            Kor Outfitter     {W}{W}  2.0      Creature — Kor Soldier   \n",
       "2                   Spirit        NaN  0.0     Token Creature — Spirit   \n",
       "3            Siren Lookout     {2}{U}  3.0     Creature — Siren Pirate   \n",
       "4                      Web        {G}  1.0          Enchantment — Aura   \n",
       "...                    ...        ...  ...                         ...   \n",
       "85059       Celestine Reef        NaN  0.0              Plane — Luvion   \n",
       "85060         Horned Troll     {2}{G}  3.0            Creature — Troll   \n",
       "85061  Faerie Bladecrafter     {2}{B}  3.0     Creature — Faerie Rogue   \n",
       "85062  Exultant Skymarcher  {1}{W}{W}  3.0  Creature — Vampire Soldier   \n",
       "85063         Disintegrate     {X}{R}  1.0                     Sorcery   \n",
       "\n",
       "                                             oracle_text power toughness  \\\n",
       "0               All Sliver creatures have double strike.     3         3   \n",
       "1      When ~ enters the battlefield, you may attach ...     2         2   \n",
       "2                                                 Flying     1         1   \n",
       "3      Flying\\nWhen ~ enters the battlefield, it expl...     1         2   \n",
       "4      Enchant creature (Target a creature as you cas...   NaN       NaN   \n",
       "...                                                  ...   ...       ...   \n",
       "85059  Creatures without flying or islandwalk can't a...   NaN       NaN   \n",
       "85060                                 {G}: Regenerate ~.     2         2   \n",
       "85061  Flying\\nWhenever one or more Faeries you contr...     2         2   \n",
       "85062                                             Flying     2         3   \n",
       "85063  ~ deals X damage to any target. If it's a crea...   NaN       NaN   \n",
       "\n",
       "      colors color_identity           keywords    rarity  \\\n",
       "0      ['R']          ['R']                 []  uncommon   \n",
       "1      ['W']          ['W']                 []    common   \n",
       "2      ['W']          ['W']           [Flying]    common   \n",
       "3      ['U']          ['U']  [Flying, Explore]    common   \n",
       "4      ['G']          ['G']          [Enchant]      rare   \n",
       "...      ...            ...                ...       ...   \n",
       "85059     []             []                 []      rare   \n",
       "85060  ['G']          ['G']                 []    common   \n",
       "85061  ['B']          ['B']           [Flying]      rare   \n",
       "85062  ['W']          ['W']           [Flying]    common   \n",
       "85063  ['R']          ['R']                 []    common   \n",
       "\n",
       "                                             flavor_text  \\\n",
       "0      \"A rift opened, and our arrows were abruptly s...   \n",
       "1      \"We take only what we need to survive. Believe...   \n",
       "2                                                    NaN   \n",
       "3                                                    NaN   \n",
       "4                                                    NaN   \n",
       "...                                                  ...   \n",
       "85059                                                NaN   \n",
       "85060  Sword hilts jut from some trolls' bodies where...   \n",
       "85061                                                NaN   \n",
       "85062  \"We have come at last to this holiest of holy ...   \n",
       "85063                                                NaN   \n",
       "\n",
       "                                                    text  \\\n",
       "0      Fury Sliver: [SEP] {5}{R} [SEP] Creature — Sli...   \n",
       "1      Kor Outfitter: [SEP] {W}{W} [SEP] Creature — K...   \n",
       "2      Spirit: [SEP]  [SEP] Token Creature — Spirit [...   \n",
       "3      Siren Lookout: [SEP] {2}{U} [SEP] Creature — S...   \n",
       "4      Web: [SEP] {G} [SEP] Enchantment — Aura [SEP] ...   \n",
       "...                                                  ...   \n",
       "85059  Celestine Reef: [SEP]  [SEP] Plane — Luvion [S...   \n",
       "85060  Horned Troll: [SEP] {2}{G} [SEP] Creature — Tr...   \n",
       "85061  Faerie Bladecrafter: [SEP] {2}{B} [SEP] Creatu...   \n",
       "85062  Exultant Skymarcher: [SEP] {1}{W}{W} [SEP] Cre...   \n",
       "85063  Disintegrate: [SEP] {X}{R} [SEP] Sorcery [SEP]...   \n",
       "\n",
       "                                text_prompt  \\\n",
       "0                 Fury Sliver: [SEP] {5}{R}   \n",
       "1               Kor Outfitter: [SEP] {W}{W}   \n",
       "2                            Spirit: [SEP]    \n",
       "3               Siren Lookout: [SEP] {2}{U}   \n",
       "4                            Web: [SEP] {G}   \n",
       "...                                     ...   \n",
       "85059                Celestine Reef: [SEP]    \n",
       "85060            Horned Troll: [SEP] {2}{G}   \n",
       "85061     Faerie Bladecrafter: [SEP] {2}{B}   \n",
       "85062  Exultant Skymarcher: [SEP] {1}{W}{W}   \n",
       "85063            Disintegrate: [SEP] {X}{R}   \n",
       "\n",
       "                                        card_description  \n",
       "0      Creature — Sliver [SEP] All Sliver creatures h...  \n",
       "1      Creature — Kor Soldier [SEP] When ~ enters the...  \n",
       "2                   Token Creature — Spirit [SEP] Flying  \n",
       "3      Creature — Siren Pirate [SEP] Flying\\nWhen ~ e...  \n",
       "4      Enchantment — Aura [SEP] Enchant creature (Tar...  \n",
       "...                                                  ...  \n",
       "85059  Plane — Luvion [SEP] Creatures without flying ...  \n",
       "85060          Creature — Troll [SEP] {G}: Regenerate ~.  \n",
       "85061  Creature — Faerie Rogue [SEP] Flying\\nWhenever...  \n",
       "85062            Creature — Vampire Soldier [SEP] Flying  \n",
       "85063  Sorcery [SEP] ~ deals X damage to any target. ...  \n",
       "\n",
       "[82351 rows x 15 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For ZY no csv read\n",
    "#with open('mtgdata.pickle', 'rb') as file:\n",
    "#    mtg_df=pickle.load(file)\n",
    "\n",
    "mtg_df=pd.read_csv('mtg_data.csv', index_col=0)\n",
    "mtg_df=mtg_df.dropna(subset=['text_prompt', 'card_description'])\n",
    "mtg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of prompts is 82351\n",
      "length of descriptions is 82351\n"
     ]
    }
   ],
   "source": [
    "#pre-processing to get rid of unregonizable characters\n",
    "rare_char={\n",
    "    '¡®°²½˝̶π’„•…™−∞☐œŠ':'',\n",
    "    'Äàáâãä':'a',\n",
    "    'Éèéêë':'e',\n",
    "    'Ææ':'ae',\n",
    "    'Óóö':'o',\n",
    "    'úûü':'u',\n",
    "    'íī':'i',\n",
    "    'Ññ':'n'\n",
    "}\n",
    "for rarechar, target in rare_char.items():\n",
    "    for char in [*rarechar]:\n",
    "        mtg_df['text_prompt']=mtg_df['text_prompt'].str.replace(char, target)\n",
    "        mtg_df['card_description']=mtg_df['card_description'].str.replace(char, target)\n",
    "\n",
    "prompt_list=list(mtg_df['text_prompt'])\n",
    "text_list=list(mtg_df['card_description'])\n",
    "print(f'length of prompts is {len(prompt_list)}\\nlength of descriptions is {len(text_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text_list=[torch.Tensor(encode(text)) for text in text_list]\n",
    "max_len=max([len(item) for item in encoded_text_list])\n",
    "padded_text_list=[torch.cat((item, torch.full((max_len - len(item),), 3))) for item in encoded_text_list] # the [PAD] token has id=3\n",
    "padded_text_list_with_CLS = [torch.cat((torch.tensor([1]), item)) for item in padded_text_list]\n",
    "\n",
    "encoded_prompt_list=[torch.Tensor(encode(text)) for text in prompt_list]\n",
    "max_prompt_len=max([len(item) for item in encoded_prompt_list])\n",
    "padded_prompt_list=[torch.cat((item, torch.full((max_prompt_len - len(item),), 3)))[:prompt_size-1] for item in encoded_prompt_list] # the [PAD] token has id=3\n",
    "padded_prompt_list_with_CLS = [torch.cat((torch.tensor([1]), item)) for item in padded_prompt_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pad_sequence(padded_text_list, batch_first=True).long()\n",
    "prompts = pad_sequence(padded_prompt_list_with_CLS, batch_first=True).long()\n",
    "n_train = int(0.9*data.shape[0])\n",
    "train_data = data[:n_train]\n",
    "val_data = data[n_train:]\n",
    "train_prompts = prompts[:n_train]\n",
    "val_prompts = prompts[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   1, 5951,  506,   29,    2,   87,   33,   89,    3,    3,    3,    3,\n",
       "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "           3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_prompts[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    #generates a small batch of data input x and target y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.stack([torch.randint(data.shape[0], (batch_size, )), torch.randint(sampling_size, (batch_size, ))]).T\n",
    "    x = torch.stack(tuple(data[i[0]][i[1]:i[1] + block_size] for i in ix))\n",
    "    y = torch.stack(tuple(data[i[0]][i[1] + 1:i[1] + block_size + 1] for i in ix))\n",
    "\n",
    "    prompt_data = train_prompts if split == 'train' else val_prompts\n",
    "    x_prompt = torch.stack(tuple(prompt_data[i[0]] for i in ix))\n",
    "    x_prompt = torch.cat((x_prompt, x), dim=-1)\n",
    "    x_prompt = x_prompt.to(device)\n",
    "    y = y.to(device)\n",
    "\n",
    "    return x_prompt, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out={}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split]=losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "\n",
    "class MaskedHead(nn.Module):\n",
    "    #one self attention head\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias= False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        # compute attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * C**0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T]==0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "        v=self.value(x)\n",
    "        out=wei @ v\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MaskedMultiHeadAttention(nn.Module):\n",
    "    \"\"\"multi head attention\"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads=nn.ModuleList([MaskedHead(head_size) for _ in range(num_heads)])\n",
    "        self.proj=nn.Linear(head_size*num_heads, n_embd)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return self.dropout(out)\n",
    "    \n",
    "class Head(nn.Module):\n",
    "    #one self attention head (unmasked)\n",
    "\n",
    "    def __init__(self, encoder_head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(encoder_n_embd, encoder_head_size, bias=False)\n",
    "        self.query = nn.Linear(encoder_n_embd, encoder_head_size, bias= False)\n",
    "        self.value = nn.Linear(encoder_n_embd, encoder_head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        # compute attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * C**0.5\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        v=self.value(x)\n",
    "        out=wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"multi head attention (unmasked)\"\"\"\n",
    "    def __init__(self, encoder_num_heads, encoder_head_size):\n",
    "        super().__init__()\n",
    "        self.heads=nn.ModuleList([Head(encoder_head_size) for _ in range(encoder_num_heads)])\n",
    "        self.proj=nn.Linear(encoder_head_size*encoder_num_heads, encoder_n_embd)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "        return self.dropout(out)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"simple feedforward perceptron layer\"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net=nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    \"\"\"Transformer encoder block: multihead self attention, followed by feedforward in to k and v\"\"\"\n",
    "    def __init__(self, encoder_n_embd, encoder_n_head):\n",
    "        super().__init__()\n",
    "        encoder_head_size=encoder_n_embd//encoder_n_head\n",
    "        self.selfattention=MultiHeadAttention(encoder_n_head, encoder_head_size)\n",
    "        self.ffwd=FeedForward(encoder_n_embd)\n",
    "        self.ln1=nn.LayerNorm(encoder_n_embd)\n",
    "        self.ln2=nn.LayerNorm(encoder_n_embd)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x+self.sa(self.ln1(x))\n",
    "        x = x+self.ffwd(self.ln2(x))\n",
    "\n",
    "class CrossAttentionHead(nn.Module):\n",
    "    \"\"\"Cross attention block: takes encoder embeddings and decoder embeddings to generate cross attention\"\"\"\n",
    "    def __init__(self, ca_head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(encoder_n_embd, ca_head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, ca_head_size, bias= False)\n",
    "        self.value = nn.Linear(encoder_n_embd, ca_head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, embedded_x_prompt):\n",
    "        x=embedded_x_prompt[:, block_size:, :]\n",
    "        prompt=embedded_x_prompt[:, :block_size, :]\n",
    "        B, T, C = x.shape\n",
    "        B_, T_, C_ = prompt.shape\n",
    "        k = self.key(prompt)\n",
    "        q = self.query(x)\n",
    "        # compute attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * C**0.5\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        v=self.value(prompt)\n",
    "        out=wei @ v\n",
    "\n",
    "        return out\n",
    "\n",
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads=nn.ModuleList([CrossAttentionHead(head_size) for _ in range(num_heads)])\n",
    "        self.proj=nn.Linear(head_size*num_heads, n_embd)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, embedded_x_prompt):\n",
    "        out = torch.cat([head(embedded_x_prompt) for head in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return torch.cat((embedded_x_prompt[:, :block_size, :], out), dim=-2)\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"Transformer decoder block: multihead self attention followed by one Feedforward layer, followed by cross-attention, followed by ffwd\"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd//n_head\n",
    "        self.sa=MaskedMultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd=FeedForward(n_embd)\n",
    "        self.ln1=nn.LayerNorm(n_embd)\n",
    "        self.ln2=nn.LayerNorm(n_embd)\n",
    "        self.ln3=nn.LayerNorm(n_embd)\n",
    "\n",
    "        self.crossattention=MultiHeadCrossAttention(n_head, head_size) # cross attention module\n",
    "    \n",
    "    def forward(self, embedded_x_prompt):\n",
    "\n",
    "        x=embedded_x_prompt[:, block_size:, :]\n",
    "        x_sa = x+self.sa(self.ln1(x))\n",
    "        x_ca = x_sa+self.crossattention(self.ln2(torch.cat((embedded_x_prompt[:, :block_size, :], x_sa), dim=-2)))[:, block_size:, :] # do cross attention with output of self attention\n",
    "        out_x = x_ca+self.ffwd(self.ln3(x_ca))\n",
    "        out_x_prompt=embedded_x_prompt[:, :block_size, :]\n",
    "\n",
    "        \"\"\"Cross Attention + Feed forward\"\"\"\n",
    "\n",
    "        return torch.cat((out_x_prompt, out_x), dim=-2)\n",
    "\n",
    "\n",
    "class MTGCardGenerator(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table=nn.Embedding(vocab_size, n_embd) #each token directly look up the logit of the next token from a lookup table\n",
    "        self.lmhead=nn.Linear(n_embd, vocab_size)\n",
    "        self.position_embedding_table=nn.Embedding(block_size, n_embd) #each token gets a position embeding of block_size, stores the relative position of token in the block\n",
    "\n",
    "        self.encoder_token_embedding_table=nn.Embedding(vocab_size, encoder_n_embd)\n",
    "        self.encoder_postion_embedding_table=nn.Embedding(vocab_size, encoder_n_embd)\n",
    "\n",
    "        self.block=nn.Sequential(*[DecoderBlock(n_embd, n_head=n_heads) for _ in range(n_layers)])\n",
    "    \n",
    "    def forward(self, x_prompt, targets=None, mode=\"train\"):\n",
    "\n",
    "        prompt=x_prompt[:,:block_size]\n",
    "        idx=x_prompt[:,block_size:]\n",
    "        \n",
    "        B, T = idx.shape\n",
    "        B_, T_ = prompt.shape\n",
    "\n",
    "        #idx and targets are both (B,T) tensors of integers, where B=batch number, T=position in batch\n",
    "        token_embeddings=self.token_embedding_table(idx) #look up value corresponding to own position in the token embedding table to form C (channel value)\n",
    "        position_embeddings=self.position_embedding_table(torch.arange(T, device=device)) #add position embeddings to token embedding\n",
    "        x= token_embeddings + position_embeddings\n",
    "        encoder_token_embeddings=self.encoder_postion_embedding_table(prompt)\n",
    "        encoder_position_embeddings=self.encoder_postion_embedding_table(torch.arange(T_, device=device))\n",
    "        prompt_x = encoder_token_embeddings+encoder_position_embeddings\n",
    "\n",
    "        embedded_x_prompt=torch.cat((prompt_x, x), dim=-2)\n",
    "        #returned_x = self.block(embedded_x_prompt)[1]\n",
    "        logits=self.lmhead(self.block(embedded_x_prompt)[:,block_size:,:])\n",
    "\n",
    "        if targets is None:\n",
    "            loss=None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            #logits are therefore values associated with each character\n",
    "            loss=F.cross_entropy(logits, targets) #evaluate loss\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, context, max_new_tokens):\n",
    "        prompt=context[:,:block_size]\n",
    "        idx=context[:,block_size:]\n",
    "        for i in range(max_new_tokens):\n",
    "            if idx.shape[-1]>block_size:\n",
    "            #crop idx to max block size\n",
    "                idx_cond=idx[:, -block_size:]\n",
    "            else:\n",
    "                idx_cond=idx\n",
    "            #get the predictions\n",
    "            logits, loss = self(torch.cat((prompt, idx_cond), dim=-1))\n",
    "            #use logits only, focus only on last time step\n",
    "            logits = logits[:, -1, :] #keep only last time step ---> (B, C)\n",
    "            #apply softmax on logit to get distribution\n",
    "            probs = F.softmax(logits, dim=-1) #get a (B, C) matrix of probabilities, sum(prob) of each B = 1\n",
    "            #sample from the distribution\n",
    "            idx_next=torch.multinomial(probs, num_samples=1) #get a (B, 1) array of predictions\n",
    "            #append prediction to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) #now a (B, T+1) matrix of returned results\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MTGCardGenerator()\n",
    "m=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path='mtggenerator_v5_check.pt'\n",
    "model=MTGCardGenerator()\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cuda')))\n",
    "model.eval()\n",
    "m=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss: 10.7375, val loss: 10.7299\n",
      "step 300: train loss: 2.9598, val loss: 2.9356\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hammerhao\\OneDrive\\Documents\\GitHub\\SpellCrafted\\v5_build.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hammerhao/OneDrive/Documents/GitHub/SpellCrafted/v5_build.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m xb_prompt, yb\u001b[39m=\u001b[39m get_batch(\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hammerhao/OneDrive/Documents/GitHub/SpellCrafted/v5_build.ipynb#X14sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m#evaluate the loss\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/hammerhao/OneDrive/Documents/GitHub/SpellCrafted/v5_build.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m logits, loss \u001b[39m=\u001b[39m model(xb_prompt, yb)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hammerhao/OneDrive/Documents/GitHub/SpellCrafted/v5_build.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad(set_to_none\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/hammerhao/OneDrive/Documents/GitHub/SpellCrafted/v5_build.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\hammerhao\\OneDrive\\Documents\\GitHub\\SpellCrafted\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\hammerhao\\OneDrive\\Documents\\GitHub\\SpellCrafted\\v5_build.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/hammerhao/OneDrive/Documents/GitHub/SpellCrafted/v5_build.ipynb#X14sZmlsZQ%3D%3D?line=213'>214</a>\u001b[0m embedded_x_prompt\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mcat((prompt_x, x), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/hammerhao/OneDrive/Documents/GitHub/SpellCrafted/v5_build.ipynb#X14sZmlsZQ%3D%3D?line=214'>215</a>\u001b[0m \u001b[39m#returned_x = self.block(embedded_x_prompt)[1]\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/hammerhao/OneDrive/Documents/GitHub/SpellCrafted/v5_build.ipynb#X14sZmlsZQ%3D%3D?line=215'>216</a>\u001b[0m logits\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlmhead(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblock(embedded_x_prompt)[:,block_size:,:])\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/hammerhao/OneDrive/Documents/GitHub/SpellCrafted/v5_build.ipynb#X14sZmlsZQ%3D%3D?line=217'>218</a>\u001b[0m \u001b[39mif\u001b[39;00m targets \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/hammerhao/OneDrive/Documents/GitHub/SpellCrafted/v5_build.ipynb#X14sZmlsZQ%3D%3D?line=218'>219</a>\u001b[0m     loss\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\hammerhao\\OneDrive\\Documents\\GitHub\\SpellCrafted\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\hammerhao\\OneDrive\\Documents\\GitHub\\SpellCrafted\\.venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\hammerhao\\OneDrive\\Documents\\GitHub\\SpellCrafted\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\hammerhao\\OneDrive\\Documents\\GitHub\\SpellCrafted\\v5_build.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/hammerhao/OneDrive/Documents/GitHub/SpellCrafted/v5_build.ipynb#X14sZmlsZQ%3D%3D?line=173'>174</a>\u001b[0m x\u001b[39m=\u001b[39membedded_x_prompt[:, block_size:, :]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/hammerhao/OneDrive/Documents/GitHub/SpellCrafted/v5_build.ipynb#X14sZmlsZQ%3D%3D?line=174'>175</a>\u001b[0m x_sa \u001b[39m=\u001b[39m x\u001b[39m+\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msa(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln1(x))\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/hammerhao/OneDrive/Documents/GitHub/SpellCrafted/v5_build.ipynb#X14sZmlsZQ%3D%3D?line=175'>176</a>\u001b[0m x_ca \u001b[39m=\u001b[39m x_sa\u001b[39m+\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcrossattention(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln2(torch\u001b[39m.\u001b[39;49mcat((embedded_x_prompt[:, :block_size, :], x_sa), dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m2\u001b[39;49m)))[:, block_size:, :] \u001b[39m# do cross attention with output of self attention\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/hammerhao/OneDrive/Documents/GitHub/SpellCrafted/v5_build.ipynb#X14sZmlsZQ%3D%3D?line=176'>177</a>\u001b[0m out_x \u001b[39m=\u001b[39m x_ca\u001b[39m+\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mffwd(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln3(x_ca))\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/hammerhao/OneDrive/Documents/GitHub/SpellCrafted/v5_build.ipynb#X14sZmlsZQ%3D%3D?line=177'>178</a>\u001b[0m out_x_prompt\u001b[39m=\u001b[39membedded_x_prompt[:, :block_size, :]\n",
      "File \u001b[1;32mc:\\Users\\hammerhao\\OneDrive\\Documents\\GitHub\\SpellCrafted\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\hammerhao\\OneDrive\\Documents\\GitHub\\SpellCrafted\\v5_build.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/hammerhao/OneDrive/Documents/GitHub/SpellCrafted/v5_build.ipynb#X14sZmlsZQ%3D%3D?line=152'>153</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, embedded_x_prompt):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/hammerhao/OneDrive/Documents/GitHub/SpellCrafted/v5_build.ipynb#X14sZmlsZQ%3D%3D?line=153'>154</a>\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([head(embedded_x_prompt) \u001b[39mfor\u001b[39;00m head \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/hammerhao/OneDrive/Documents/GitHub/SpellCrafted/v5_build.ipynb#X14sZmlsZQ%3D%3D?line=154'>155</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj(out))\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/hammerhao/OneDrive/Documents/GitHub/SpellCrafted/v5_build.ipynb#X14sZmlsZQ%3D%3D?line=155'>156</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat((embedded_x_prompt[:, :block_size, :], out), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\hammerhao\\OneDrive\\Documents\\GitHub\\SpellCrafted\\v5_build.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/hammerhao/OneDrive/Documents/GitHub/SpellCrafted/v5_build.ipynb#X14sZmlsZQ%3D%3D?line=152'>153</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, embedded_x_prompt):\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/hammerhao/OneDrive/Documents/GitHub/SpellCrafted/v5_build.ipynb#X14sZmlsZQ%3D%3D?line=153'>154</a>\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([head(embedded_x_prompt) \u001b[39mfor\u001b[39;00m head \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/hammerhao/OneDrive/Documents/GitHub/SpellCrafted/v5_build.ipynb#X14sZmlsZQ%3D%3D?line=154'>155</a>\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj(out))\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/hammerhao/OneDrive/Documents/GitHub/SpellCrafted/v5_build.ipynb#X14sZmlsZQ%3D%3D?line=155'>156</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat((embedded_x_prompt[:, :block_size, :], out), dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\hammerhao\\OneDrive\\Documents\\GitHub\\SpellCrafted\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\hammerhao\\OneDrive\\Documents\\GitHub\\SpellCrafted\\v5_build.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/hammerhao/OneDrive/Documents/GitHub/SpellCrafted/v5_build.ipynb#X14sZmlsZQ%3D%3D?line=138'>139</a>\u001b[0m wei \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(wei)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/hammerhao/OneDrive/Documents/GitHub/SpellCrafted/v5_build.ipynb#X14sZmlsZQ%3D%3D?line=140'>141</a>\u001b[0m v\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue(prompt)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/hammerhao/OneDrive/Documents/GitHub/SpellCrafted/v5_build.ipynb#X14sZmlsZQ%3D%3D?line=141'>142</a>\u001b[0m out\u001b[39m=\u001b[39mwei \u001b[39m@\u001b[39;49m v\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/hammerhao/OneDrive/Documents/GitHub/SpellCrafted/v5_build.ipynb#X14sZmlsZQ%3D%3D?line=143'>144</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\hammerhao\\OneDrive\\Documents\\GitHub\\SpellCrafted\\.venv\\lib\\site-packages\\torch\\fx\\traceback.py:41\u001b[0m, in \u001b[0;36mformat_stack\u001b[1;34m()\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[39mreturn\u001b[39;00m [current_meta\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mstack_trace\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)]\n\u001b[0;32m     39\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     40\u001b[0m     \u001b[39m# fallback to traceback.format_stack()\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     \u001b[39mreturn\u001b[39;00m traceback\u001b[39m.\u001b[39mformat_list(traceback\u001b[39m.\u001b[39;49mextract_stack()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\traceback.py:227\u001b[0m, in \u001b[0;36mextract_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    226\u001b[0m     f \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39m_getframe()\u001b[39m.\u001b[39mf_back\n\u001b[1;32m--> 227\u001b[0m stack \u001b[39m=\u001b[39m StackSummary\u001b[39m.\u001b[39;49mextract(walk_stack(f), limit\u001b[39m=\u001b[39;49mlimit)\n\u001b[0;32m    228\u001b[0m stack\u001b[39m.\u001b[39mreverse()\n\u001b[0;32m    229\u001b[0m \u001b[39mreturn\u001b[39;00m stack\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\traceback.py:379\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    376\u001b[0m     result\u001b[39m.\u001b[39mappend(FrameSummary(\n\u001b[0;32m    377\u001b[0m         filename, lineno, name, lookup_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39mlocals\u001b[39m\u001b[39m=\u001b[39mf_locals))\n\u001b[0;32m    378\u001b[0m \u001b[39mfor\u001b[39;00m filename \u001b[39min\u001b[39;00m fnames:\n\u001b[1;32m--> 379\u001b[0m     linecache\u001b[39m.\u001b[39;49mcheckcache(filename)\n\u001b[0;32m    380\u001b[0m \u001b[39m# If immediate lookup was desired, trigger lookups now.\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[39mif\u001b[39;00m lookup_lines:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\linecache.py:72\u001b[0m, in \u001b[0;36mcheckcache\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[39mcontinue\u001b[39;00m   \u001b[39m# no-op for files loaded via a __loader__\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 72\u001b[0m     stat \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mstat(fullname)\n\u001b[0;32m     73\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     cache\u001b[39m.\u001b[39mpop(filename, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "optimizer=torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "    # every once in a while evaluate the loss of train and val\n",
    "    if iter % eval_interval == 0:\n",
    "        losses=estimate_loss()\n",
    "        print(f\"step {iter}: train loss: {losses['train']:.4f}, val loss: {losses['val']:.4f}\")\n",
    "    \n",
    "    #sample a batch of data\n",
    "    xb_prompt, yb= get_batch('train')\n",
    "\n",
    "    #evaluate the loss\n",
    "    logits, loss = model(xb_prompt, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(m.state_dict(), 'mtggenerator_v5.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'party { T }: Add { B } ( You may cast this card for its of any name'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate(cardname, mana):\n",
    "    prompt= torch.tensor([encode(f'[CLS] {cardname}: [SEP] {mana}')], dtype=torch.long, device=device)\n",
    "    padding_values = torch.full((1, prompt_size-prompt.shape[-1]), 3, dtype=torch.long, device=device)\n",
    "    padded_prompt = torch.cat((prompt, padding_values), dim=-1)\n",
    "    start = torch.tensor([encode('[CLS]')], dtype=torch.long, device=device)\n",
    "    context=torch.cat((padded_prompt, start), dim=-1)\n",
    "    response=m.generate(context, max_new_tokens=20)[0].tolist()\n",
    "    return decode(response)\n",
    "generate('The Big Bang', '{2}{R}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spell or dealt damage to each attacking .'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate('High Tide', '{2}{U}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cost': 'hello Van Darkholme Cost',\n",
       " 'description': 'hello My name is Van',\n",
       " 'name': 'hello Van Darkholme Name'}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "requests.get('http://107.22.21.89/', params={'prompt':'hello'}).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
